{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IqM-T1RTzY6C"
      },
      "source": [
        "# Text classification with Unsloth\n",
        "\n",
        "This modified Unsloth notebook trains an LLM on any text classification dataset, where the input is a csv with columns \"text\" and \"label\".\n",
        "\n",
        "### Added features:\n",
        "\n",
        "- Trims the classification head to contain only the number tokens such as \"1\", \"2\" etc, which saves 1 GB of VRAM, allows you to train the head without massive memory usage, and makes the start of the training session more stable.\n",
        "- Only the last token in the sequence contributes to the loss, the model doesn't waste its capacity by trying to predict the input\n",
        "- includes \"group_by_length = True\" which speeds up training significantly for unbalanced sequence lengths\n",
        "- Efficiently evaluates the accuracy on the validation set using batched inference\n",
        "\n",
        "### Update 4th of May 2025:\n",
        "\n",
        "- Added support for more than 2 classes\n",
        "- The classification head is now built back up to the original size after training, no more errors in external libraries.\n",
        "- Made the batched inference part much faster and cleaner\n",
        "- Changed model to Qwen 3\n",
        "- Improved comments to explain the complicated parts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Si0l5tIjpWe1",
        "outputId": "e197d99e-5e57-456d-95a3-312103173b02"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ü¶• Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
            "ü¶• Unsloth Zoo will now patch everything to make training faster!\n"
          ]
        }
      ],
      "source": [
        "# needed as this function doesn't like it when the lm_head has its size changed\n",
        "from unsloth import tokenizer_utils\n",
        "def do_nothing(*args, **kwargs):\n",
        "    pass\n",
        "tokenizer_utils.fix_untrained_tokens = do_nothing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 194,
          "referenced_widgets": [
            "6c29cff5c3b44b47a3c62e532168b0a6",
            "cc2dfedf5a0b4bf4b58d2e999112d476",
            "6fc2e950e7204b6fa1f25f9d3b2d0a92",
            "28bda91e30d944d39e57f1b9b1bd5b52",
            "ab83c0c74bc24e3e96a49a602430a862",
            "7e563ea835964ef7b3876f5deab1e725",
            "f6ff88e103754871b310ba3f3a433449",
            "0ca2418d9885484ca6dbf3c90ff18183",
            "0d142c8937f64fee97e0cad57e3c0be4",
            "49772c6f304544ad86ad9f03c955c213",
            "2d86f18e5c3841df95489b5de7c7411c"
          ]
        },
        "id": "v-wGKQi4pWe2",
        "outputId": "985e7230-3add-4217-9132-e54da7a1b564"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Major: 7, Minor: 5\n",
            "==((====))==  Unsloth 2025.8.7: Fast Qwen3 patching. Transformers: 4.55.1.\n",
            "   \\\\   /|    Tesla T4. Num GPUs = 1. Max memory: 14.741 GB. Platform: Linux.\n",
            "O^O/ \\_/ \\    Torch: 2.6.0+cu124. CUDA: 7.5. CUDA Toolkit: 12.4. Triton: 3.2.0\n",
            "\\        /    Bfloat16 = FALSE. FA [Xformers = None. FA2 = False]\n",
            " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
            "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "6c29cff5c3b44b47a3c62e532168b0a6"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "import torch\n",
        "major_version, minor_version = torch.cuda.get_device_capability()\n",
        "print(f\"Major: {major_version}, Minor: {minor_version}\")\n",
        "from datasets import load_dataset\n",
        "import datasets\n",
        "from trl import SFTTrainer\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from unsloth import FastLanguageModel\n",
        "from trl import SFTTrainer, SFTConfig\n",
        "from transformers import TrainingArguments, Trainer\n",
        "from typing import Tuple\n",
        "import warnings\n",
        "from typing import Any, Dict, List, Union\n",
        "from transformers import DataCollatorForLanguageModeling\n",
        "from sklearn.model_selection import train_test_split\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "NUM_CLASSES = 3 # number of classes in the csv\n",
        "\n",
        "max_seq_length = 2048 # Choose any! We auto support RoPE Scaling internally!\n",
        "dtype = None # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\n",
        "\n",
        "model_name = \"unsloth/Qwen3-4B-Base\";load_in_4bit = False\n",
        "# model_name = \"Qwen3-4B-Base\";load_in_4bit = False\n",
        "\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name = model_name,load_in_4bit = load_in_4bit,\n",
        "    max_seq_length = max_seq_length,\n",
        "    dtype = dtype,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SXd9bTZd1aaL"
      },
      "source": [
        "We now trim the classification head so the model can only say numbers 0-NUM_CLASSES and no other words. (We don't use 0 here but keeping it makes everything simpler)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gn-Gd4MPpWe3",
        "outputId": "ba5f7ae5-cbc7-4064-a04a-a2381a79188b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "allowed ids: [16, 17, 18]\n"
          ]
        }
      ],
      "source": [
        "# ‚úÖ ÏïàÏ†ÑÌïú lm_head Ï∂ïÏÜå (0 Ï†úÍ±∞, bias/ÎîîÎ∞îÏù¥Ïä§/dtypeÍπåÏßÄ Î≥¥Ï°¥)\n",
        "ALLOWED_DIGITS = list(range(1, NUM_CLASSES+1))  # 1..NUM_CLASSES\n",
        "number_token_ids = [tokenizer.encode(str(i), add_special_tokens=False)[0] for i in ALLOWED_DIGITS]\n",
        "\n",
        "# Î≥µÍµ¨Ïö©ÏúºÎ°ú ÏõêÎûò vocab ÌÅ¨Í∏∞ Î≥¥Í¥Ä (ÎÇòÏ§ëÏóê ÏõêÎ≥µ Îã®Í≥ÑÏóêÏÑú ÏÇ¨Ïö©)\n",
        "old_size   = model.lm_head.weight.shape[0]\n",
        "hidden_dim = model.lm_head.weight.shape[1]\n",
        "dev        = model.lm_head.weight.device\n",
        "dtype      = model.lm_head.weight.dtype\n",
        "has_bias   = hasattr(model.lm_head, \"bias\") and (model.lm_head.bias is not None)\n",
        "\n",
        "with torch.no_grad():\n",
        "    idx = torch.tensor(number_token_ids, device=dev, dtype=torch.long)\n",
        "    w_small = model.lm_head.weight.data.index_select(0, idx).contiguous()\n",
        "    if has_bias:\n",
        "        b_small = model.lm_head.bias.data.index_select(0, idx).contiguous()\n",
        "\n",
        "    new_head = torch.nn.Linear(hidden_dim, len(number_token_ids), bias=has_bias, device=dev, dtype=dtype)\n",
        "    new_head.weight.data.copy_(w_small)\n",
        "    if has_bias:\n",
        "        new_head.bias.data.copy_(b_small)\n",
        "\n",
        "# üîÅ Ï†ÑÏ≤¥ Î†àÏù¥Ïñ¥Î•º ÍµêÏ≤¥ (weightÎßå Í∞àÏßÄ ÏïäÏùå!)\n",
        "model.lm_head = new_head\n",
        "\n",
        "# ÎùºÎ≤® Ïû¨Îß§Ìïë(ÏõêÎûò ÌÜ†ÌÅ∞ id -> Ï∂ïÏÜå Ìó§Îìú Ïù∏Îç±Ïä§)\n",
        "reverse_map = {tok_id: i for i, tok_id in enumerate(number_token_ids)}\n",
        "print(\"allowed ids:\", number_token_ids)\n",
        "\n",
        "# (ÏÑ†ÌÉù) meta ÌÖêÏÑú Ï°¥Ïû¨ ÌôïÏù∏\n",
        "bad = [n for n,p in model.named_parameters() if p.device.type == \"meta\"]\n",
        "assert not bad, f\"Meta tensors found: {bad}\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BsblDvGQpWe3",
        "outputId": "fbf0c5fc-e5f3-4ebc-d9e6-0dd0a29ceaeb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Unsloth: Offloading output_embeddings to disk to save VRAM\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Unsloth 2025.8.7 patched 36 layers with 36 QKV layers, 36 O layers and 36 MLP layers.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Unsloth: Training lm_head in mixed precision to save VRAM\n",
            "trainable parameters: 33037824\n"
          ]
        }
      ],
      "source": [
        "from peft import LoftQConfig\n",
        "\n",
        "model = FastLanguageModel.get_peft_model(\n",
        "    model,\n",
        "    r = 16,\n",
        "    target_modules = [\n",
        "        \"lm_head\", # can easily be trained because it now has a small size\n",
        "        \"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
        "        \"gate_proj\", \"up_proj\", \"down_proj\",],\n",
        "    lora_alpha = 16,\n",
        "    lora_dropout = 0, # Supports any, but = 0 is optimized\n",
        "    bias = \"none\",    # Supports any, but = \"none\" is optimized\n",
        "    use_gradient_checkpointing = \"unsloth\",\n",
        "    random_state = 3407,\n",
        "    use_rslora = True,  # We support rank stabilized LoRA\n",
        "    # init_lora_weights = 'loftq',\n",
        "    # loftq_config = LoftQConfig(loftq_bits = 4, loftq_iter = 1), # And LoftQ\n",
        ")\n",
        "print(\"trainable parameters:\", sum(p.numel() for p in model.parameters() if p.requires_grad))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XIaj0hUupWe3"
      },
      "source": [
        "The dataset can be found [here](https://github.com/timothelaborie/text_classification_scripts/blob/main/data/finance_sentiment_multiclass.csv)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9ODQcMlmpWe3",
        "outputId": "be6fd6be-51b4-44ce-ad45-a9d9d2559ad2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Labels: [np.int64(1), np.int64(2), np.int64(3)]\n"
          ]
        }
      ],
      "source": [
        "kaggle = os.getcwd() == \"/kaggle/working\"\n",
        "candidates = [\n",
        "    \"/kaggle/input/whatever/finance_sentiment_multiclass.csv\",\n",
        "    \"data/finance_sentiment_multiclass.csv\",\n",
        "    \"./finance_sentiment_multiclass.csv\",\n",
        "]\n",
        "csv_path = next((p for p in candidates if os.path.exists(p)), None)\n",
        "assert csv_path is not None, f\"CSV not found in: {candidates}\"\n",
        "data = pd.read_csv(csv_path)\n",
        "\n",
        "# Ïª¨Îüº/ÎùºÎ≤® Í≤ÄÏ¶ù\n",
        "assert {\"text\",\"label\"}.issubset(set(data.columns)), f\"Missing columns in {csv_path}\"\n",
        "print(\"Labels:\", sorted(data['label'].unique()))\n",
        "# ÎßåÏïΩ ÎùºÎ≤®Ïù¥ 0/1/2ÎùºÎ©¥ -> 1/2/3ÏúºÎ°ú Ïò¨Î¶¨Í∏∞ (ÌîÑÎ°¨ÌîÑÌä∏ ÎßûÏ∂§)\n",
        "if set(data['label'].unique()) == {0,1,2}:\n",
        "    data['label'] = data['label'] + 1\n",
        "\n",
        "train_df, val_df = train_test_split(data, test_size=0.1, random_state=42, stratify=data['label'])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 431
        },
        "id": "AkqqlRfNpWe4",
        "outputId": "5f9905dc-dc7d-41ee-a6cf-180c88f43eec"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "token_counts = [len(tokenizer.encode(x)) for x in train_df.text]\n",
        "# plot the token counts\n",
        "a = plt.hist(token_counts, bins=30)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "LjY75GoYUCB8"
      },
      "outputs": [],
      "source": [
        "prompt = \"\"\"Here is a financial news:\n",
        "{}\n",
        "\n",
        "Classify this news into one of the following:\n",
        "class 1: Bullish\n",
        "class 2: Neutral\n",
        "class 3: Bearish\n",
        "\n",
        "SOLUTION\n",
        "The correct answer is: class {}\"\"\"\n",
        "\n",
        "def formatting_prompts_func(dataset_):\n",
        "    texts = []\n",
        "    for i in range(len(dataset_['text'])):\n",
        "        text_ = dataset_['text'].iloc[i]\n",
        "        label_ = dataset_['label'].iloc[i] # the csv is setup so that the label column corresponds exactly to the 3 classes defined above in the prompt (important)\n",
        "\n",
        "        text = prompt.format(text_, label_)\n",
        "\n",
        "        texts.append(text)\n",
        "    return texts\n",
        "\n",
        "# apply formatting_prompts_func to train_df\n",
        "train_df['text'] = formatting_prompts_func(train_df)\n",
        "train_dataset = datasets.Dataset.from_pandas(train_df,preserve_index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "4XavdraipWe4"
      },
      "outputs": [],
      "source": [
        "# this custom collator makes it so the model trains only on the last token of the sequence. It also maps from the old tokenizer to the new lm_head indices\n",
        "class DataCollatorForLastTokenLM(DataCollatorForLanguageModeling):\n",
        "    def __init__(\n",
        "        self,\n",
        "        *args,\n",
        "        mlm: bool = False,\n",
        "        ignore_index: int = -100,\n",
        "        **kwargs,\n",
        "    ):\n",
        "        super().__init__(*args, mlm=mlm, **kwargs)\n",
        "        self.ignore_index = ignore_index\n",
        "\n",
        "    def torch_call(self, examples: List[Union[List[int], Any, Dict[str, Any]]]) -> Dict[str, Any]:\n",
        "        batch = super().torch_call(examples)\n",
        "\n",
        "        for i in range(len(examples)):\n",
        "            # Find the last non-padding token\n",
        "            last_token_idx = (batch[\"labels\"][i] != self.ignore_index).nonzero()[-1].item()\n",
        "            # Set all labels to ignore_index except for the last token\n",
        "            batch[\"labels\"][i, :last_token_idx] = self.ignore_index\n",
        "            # If the last token in the text is, for example, \"2\", then this was processed with the old tokenizer into number_token_ids[2]\n",
        "            # But we don't actually want this because number_token_ids[2] could be something like 27, which is now undefined in the new lm_head. So we map it to the new lm_head index.\n",
        "            # if this line gives you a keyerror then increase max_seq_length\n",
        "            batch[\"labels\"][i, last_token_idx] = reverse_map[ batch[\"labels\"][i, last_token_idx].item() ]\n",
        "\n",
        "\n",
        "        return batch\n",
        "collator = DataCollatorForLastTokenLM(tokenizer=tokenizer)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "idAEIeSQ3xdS"
      },
      "source": [
        "<a name=\"Train\"></a>\n",
        "### Train the model\n",
        "Now let's use Huggingface TRL's `SFTTrainer`! More docs here: [TRL SFT docs](https://huggingface.co/docs/trl/sft_trainer). We do 60 steps to speed things up, but you can set `num_train_epochs=1` for a full run, and turn off `max_steps=None`. We also support TRL's `DPOTrainer`!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "5c68dd49b2354d2da961d3d5d4a864a3",
            "f1849fc96a074fb392d11e8559fcd063",
            "fd86ec86375a403c87a617fa9f56d685",
            "6b4d67b80e314e26b9420cd6b83bdd8b",
            "382c8cdb24554f4db11b9da6eb057ab1",
            "52c69c3af7504294bee8d85ecc0a963a",
            "fcce99d90452441a957846199e4e2d96",
            "2d51a6b7560e45d4b31a33f422446efa",
            "106ded37e9db4c19aba77b41de3eae02",
            "ccca9d950c8240c583dad8e9005f79c0",
            "cc38ef1594d9422189d15d20bcf44229"
          ]
        },
        "id": "95_Nn-89DhsL",
        "outputId": "e87ef048-1ec1-40b3-b847-70e39af52828"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Unsloth: Tokenizing [\"text\"] (num_proc=2):   0%|          | 0/3893 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "5c68dd49b2354d2da961d3d5d4a864a3"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "trainer = SFTTrainer(\n",
        "    model = model,\n",
        "    tokenizer = tokenizer,\n",
        "    train_dataset = train_dataset,\n",
        "    max_seq_length = max_seq_length,\n",
        "    dataset_num_proc = 1,\n",
        "    packing = False, # not needed because group_by_length is True\n",
        "    args = SFTConfig(\n",
        "        per_device_train_batch_size = 32,\n",
        "        gradient_accumulation_steps = 1,\n",
        "        warmup_steps = 10,\n",
        "        learning_rate = 1e-4,\n",
        "        fp16 = not torch.cuda.is_bf16_supported(),\n",
        "        bf16 = torch.cuda.is_bf16_supported(),\n",
        "        logging_steps = 1,\n",
        "        optim = \"adamw_8bit\",\n",
        "        weight_decay = 0.01,\n",
        "        lr_scheduler_type = \"cosine\",\n",
        "        seed = 3407,\n",
        "        output_dir = \"outputs\",\n",
        "        num_train_epochs = 1,\n",
        "        # report_to = \"wandb\",\n",
        "        report_to = \"none\",\n",
        "        group_by_length = True,\n",
        "    ),\n",
        "    data_collator=collator,\n",
        "    dataset_text_field=\"text\",\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2ejIt2xSNKKp",
        "outputId": "596992f8-e38d-463d-8a20-6c0fae75dab1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPU = Tesla T4. Max memory = 14.741 GB.\n",
            "7.762 GB of memory reserved.\n"
          ]
        }
      ],
      "source": [
        "#@title Show current memory stats\n",
        "gpu_stats = torch.cuda.get_device_properties(0)\n",
        "start_gpu_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
        "max_memory = round(gpu_stats.total_memory / 1024 / 1024 / 1024, 3)\n",
        "print(f\"GPU = {gpu_stats.name}. Max memory = {max_memory} GB.\")\n",
        "print(f\"{start_gpu_memory} GB of memory reserved.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 203
        },
        "id": "yqxqAZ7KJ4oL",
        "outputId": "9abf8dd5-6c05-4472-81bd-808a951cb674"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1\n",
            "   \\\\   /|    Num examples = 3,893 | Num Epochs = 1 | Total steps = 122\n",
            "O^O/ \\_/ \\    Batch size per device = 32 | Gradient accumulation steps = 1\n",
            "\\        /    Data Parallel GPUs = 1 | Total batch size (32 x 1 x 1) = 32\n",
            " \"-____-\"     Trainable parameters = 33,037,824 of 4,055,513,600 (0.81% trained)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='2' max='122' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [  2/122 : < :, Epoch 0.01/1]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Unsloth: Will smartly offload gradients to save VRAM!\n"
          ]
        }
      ],
      "source": [
        "trainer_stats = trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pCqnaKmlO1U9"
      },
      "outputs": [],
      "source": [
        "#@title Show final memory and time stats\n",
        "used_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
        "used_memory_for_lora = round(used_memory - start_gpu_memory, 3)\n",
        "used_percentage = round(used_memory         /max_memory*100, 3)\n",
        "lora_percentage = round(used_memory_for_lora/max_memory*100, 3)\n",
        "print(f\"{trainer_stats.metrics['train_runtime']} seconds used for training.\")\n",
        "print(f\"{round(trainer_stats.metrics['train_runtime']/60, 2)} minutes used for training.\")\n",
        "print(f\"Peak reserved memory = {used_memory} GB.\")\n",
        "print(f\"Peak reserved memory for training = {used_memory_for_lora} GB.\")\n",
        "print(f\"Peak reserved memory % of max memory = {used_percentage} %.\")\n",
        "print(f\"Peak reserved memory for training % of max memory = {lora_percentage} %.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ekOmTR1hSNcr"
      },
      "source": [
        "<a name=\"Inference\"></a>\n",
        "### Inference\n",
        "This part evaluates the model on the val set with batched inference"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NnZ2SUg0pWe5"
      },
      "outputs": [],
      "source": [
        "FastLanguageModel.for_inference(model) # Enable native 2x faster inference\n",
        "print()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2WKeGfRDpWe5"
      },
      "source": [
        "### remake the old lm_head but with unused tokens having -1000 bias and 0 weights (improves compatibility with libraries like vllm)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uxbld_uhpWe5"
      },
      "outputs": [],
      "source": [
        "# Save the current (trimmed) lm_head and bias\n",
        "trimmed_lm_head = model.lm_head.weight.data.clone()\n",
        "trimmed_lm_head_bias = model.lm_head.bias.data.clone() if hasattr(model.lm_head, \"bias\") and model.lm_head.bias is not None else torch.zeros(len(number_token_ids), device=trimmed_lm_head.device)\n",
        "\n",
        "# Create a new lm_head with shape [old_size, hidden_dim]\n",
        "hidden_dim = trimmed_lm_head.shape[1]\n",
        "new_lm_head = torch.full((old_size, hidden_dim), 0, dtype=trimmed_lm_head.dtype, device=trimmed_lm_head.device)\n",
        "new_lm_head_bias = torch.full((old_size,), -1000.0, dtype=trimmed_lm_head_bias.dtype, device=trimmed_lm_head_bias.device)\n",
        "\n",
        "# Fill in the weights and bias for the allowed tokens (number_token_ids)\n",
        "for new_idx, orig_token_id in enumerate(number_token_ids):\n",
        "    new_lm_head[orig_token_id] = trimmed_lm_head[new_idx]\n",
        "    new_lm_head_bias[orig_token_id] = trimmed_lm_head_bias[new_idx]\n",
        "\n",
        "# Update the model's lm_head weight and bias\n",
        "with torch.no_grad():\n",
        "    new_lm_head_module = torch.nn.Linear(hidden_dim, old_size, bias=True, device=model.device)\n",
        "    new_lm_head_module.weight.data.copy_(new_lm_head)\n",
        "    new_lm_head_module.bias.data.copy_(new_lm_head_bias)\n",
        "    model.lm_head.modules_to_save[\"default\"] = new_lm_head_module\n",
        "\n",
        "print(f\"Remade lm_head: shape = {model.lm_head.weight.shape}. Allowed tokens: {number_token_ids}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-EemHwcepWe5"
      },
      "source": [
        "# Batched Inference on Validation Set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BEkE0OOUpWe5"
      },
      "outputs": [],
      "source": [
        "import torch, torch.nn.functional as F\n",
        "from tqdm import tqdm\n",
        "import numpy as np, random\n",
        "\n",
        "def build_infer_prompt(text: str) -> str:\n",
        "    return f\"\"\"Here is a financial news:\n",
        "{text}\n",
        "\n",
        "Classify this news into one of the following:\n",
        "class 1: Bullish\n",
        "class 2: Neutral\n",
        "class 3: Bearish\n",
        "\n",
        "SOLUTION\n",
        "The correct answer is: class \"\"\"\n",
        "\n",
        "val_df['token_length'] = val_df['text'].apply(lambda x: len(tokenizer.encode(x, add_special_tokens=False)))\n",
        "val_df_sorted = val_df.sort_values(by='token_length').reset_index(drop=True)\n",
        "\n",
        "batch_size = 16\n",
        "device = model.device\n",
        "correct = 0\n",
        "all_true, all_pred = [], []\n",
        "\n",
        "with torch.inference_mode():\n",
        "    for i in tqdm(range(0, len(val_df_sorted), batch_size), desc=\"Evaluating\"):\n",
        "        batch   = val_df_sorted.iloc[i:i+batch_size]\n",
        "        prompts = [build_infer_prompt(t) for t in batch['text']]\n",
        "        inputs  = tokenizer(prompts, return_tensors=\"pt\", padding=True, truncation=True,\n",
        "                            max_length=max_seq_length).to(device)\n",
        "        logits = model(**inputs).logits\n",
        "        last_idxs   = inputs.attention_mask.sum(1) - 1\n",
        "        last_logits = logits[torch.arange(len(batch)), last_idxs, :]\n",
        "\n",
        "        # Ïà´Ïûê ÌÜ†ÌÅ∞Îßå ÏÜåÌîÑÌä∏Îß•Ïä§ ‚Üí argmax(0..N-1) ‚Üí +1 == (1..N)\n",
        "        probs = F.softmax(last_logits[:, number_token_ids], dim=-1)\n",
        "        preds = torch.argmax(probs, dim=-1).cpu().numpy() + 1\n",
        "\n",
        "        y = batch['label'].to_numpy()\n",
        "        correct += int((preds == y).sum())\n",
        "        all_true.append(y); all_pred.append(preds)\n",
        "\n",
        "accuracy = 100 * correct / len(val_df_sorted)\n",
        "print(f\"\\nValidation accuracy: {accuracy:.2f}% ({correct}/{len(val_df_sorted)})\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IWpeRNc3pWe6"
      },
      "outputs": [],
      "source": [
        "# stop running all cells\n",
        "#1/0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aSbdeKDUpWe6"
      },
      "source": [
        "Now if you closed the notebook kernel and want to reload the model:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iyDLVFOgpWe6"
      },
      "outputs": [],
      "source": [
        "# Ïòà: 16bitÎ°ú Î≥ëÌï© Ï†ÄÏû•Ìïú Îí§ Î°úÎî©\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    \"hf/model\",  # ‚Üê Ïã§Ï†ú Ï†ÄÏû• Ìè¥ÎçîÎ™Ö\n",
        "    load_in_4bit=False, max_seq_length=2048, dtype=None,\n",
        ")\n",
        "FastLanguageModel.for_inference(model)\n",
        "\n",
        "\n",
        "prompt = \"\"\"Here is a financial news:\n",
        "For the global oil market, the coronavirus epidemic couldn't have hit a worse place\n",
        "\n",
        "Classify this news into one of the following:\n",
        "class 1: Bullish\n",
        "class 2: Neutral\n",
        "class 3: Bearish\n",
        "\n",
        "SOLUTION\n",
        "The correct answer is: class \"\"\"\n",
        "inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
        "outputs = model.generate(**inputs, max_new_tokens=1, use_cache=True)\n",
        "decoded = tokenizer.batch_decode(outputs)\n",
        "print(decoded)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f422JgM9sdVT"
      },
      "source": [
        "### Saving to float16 for VLLM\n",
        "\n",
        "We also support saving to `float16` directly. Select `merged_16bit` for float16 or `merged_4bit` for int4. We also allow `lora` adapters as a fallback. Use `push_to_hub_merged` to upload to your Hugging Face account! You can go to https://huggingface.co/settings/tokens for your personal tokens."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iHjt_SMYsd3P"
      },
      "outputs": [],
      "source": [
        "# Merge to 16bit\n",
        "if False: model.save_pretrained_merged(\"hf/model\", tokenizer, save_method = \"merged_16bit\",)\n",
        "if False: model.push_to_hub_merged(\"hf/model\", tokenizer, save_method = \"merged_16bit\", token = \"\")\n",
        "\n",
        "# Merge to 4bit\n",
        "if False: model.save_pretrained_merged(\"hf/model\", tokenizer, save_method = \"merged_4bit\",)\n",
        "if False: model.push_to_hub_merged(\"hf/model\", tokenizer, save_method = \"merged_4bit\", token = \"\")\n",
        "\n",
        "# Just LoRA adapters\n",
        "if False: model.save_pretrained_merged(\"model\", tokenizer, save_method = \"lora\",)\n",
        "if False: model.push_to_hub_merged(\"hf/model\", tokenizer, save_method = \"lora\", token = \"\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TCv4vXHd61i7"
      },
      "source": [
        "### GGUF / llama.cpp Conversion\n",
        "To save to `GGUF` / `llama.cpp`, we support it natively now! We clone `llama.cpp` and we default save it to `q8_0`. We allow all methods like `q4_k_m`. Use `save_pretrained_gguf` for local saving and `push_to_hub_gguf` for uploading to HF."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FqfebeAdT073"
      },
      "outputs": [],
      "source": [
        "# Save to 8bit Q8_0\n",
        "if False: model.save_pretrained_gguf(\"model\", tokenizer,)\n",
        "if False: model.push_to_hub_gguf(\"hf/model\", tokenizer, token = \"\")\n",
        "\n",
        "# Save to 16bit GGUF\n",
        "if False: model.save_pretrained_gguf(\"model\", tokenizer, quantization_method = \"f16\")\n",
        "if False: model.push_to_hub_gguf(\"hf/model\", tokenizer, quantization_method = \"f16\", token = \"\")\n",
        "\n",
        "# Save to q4_k_m GGUF\n",
        "if False: model.save_pretrained_gguf(\"model\", tokenizer, quantization_method = \"q4_k_m\")\n",
        "if False: model.push_to_hub_gguf(\"hf/model\", tokenizer, quantization_method = \"q4_k_m\", token = \"\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bDp0zNpwe6U_"
      },
      "source": [
        "Now, use the `model-unsloth.gguf` file or `model-unsloth-Q4_K_M.gguf` file in `llama.cpp` or a UI based system like `GPT4All`. You can install GPT4All by going [here](https://gpt4all.io/index.html)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zt9CHJqO6p30"
      },
      "source": [
        "And we're done! If you have any questions on Unsloth, we have a [Discord](https://discord.gg/u54VK8m8tk) channel! If you find any bugs or want to keep updated with the latest LLM stuff, or need help, join projects etc, feel free to join our Discord!\n",
        "\n",
        "Some other links:\n",
        "1. Zephyr DPO 2x faster [free Colab](https://colab.research.google.com/drive/15vttTpzzVXv_tJwEk-hIcQ0S9FcEWvwP?usp=sharing)\n",
        "2. Llama 7b 2x faster [free Colab](https://colab.research.google.com/drive/1lBzz5KeZJKXjvivbYvmGarix9Ao6Wxe5?usp=sharing)\n",
        "3. TinyLlama 4x faster full Alpaca 52K in 1 hour [free Colab](https://colab.research.google.com/drive/1AZghoNBQaMDgWJpi4RbffGM1h6raLUj9?usp=sharing)\n",
        "4. CodeLlama 34b 2x faster [A100 on Colab](https://colab.research.google.com/drive/1y7A0AxE3y8gdj4AVkl2aZX47Xu3P1wJT?usp=sharing)\n",
        "5. Llama 7b [free Kaggle](https://www.kaggle.com/danielhanchen/unsloth-alpaca-t4-ddp)\n",
        "6. We also did a [blog](https://huggingface.co/blog/unsloth-trl) with ü§ó HuggingFace, and we're in the TRL [docs](https://huggingface.co/docs/trl/main/en/sft_trainer#accelerate-fine-tuning-2x-using-unsloth)!\n",
        "\n",
        "<div class=\"align-center\">\n",
        "  <a href=\"https://github.com/unslothai/unsloth\"><img src=\"https://github.com/unslothai/unsloth/raw/main/images/unsloth%20new%20logo.png\" width=\"115\"></a>\n",
        "  <a href=\"https://discord.gg/u54VK8m8tk\"><img src=\"https://github.com/unslothai/unsloth/raw/main/images/Discord.png\" width=\"145\"></a>\n",
        "  <a href=\"https://ko-fi.com/unsloth\"><img src=\"https://github.com/unslothai/unsloth/raw/main/images/Kofi button.png\" width=\"145\"></a></a> Support our work if you can! Thanks!\n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4e9d2beb"
      },
      "source": [
        "# Download the dataset\n",
        "!mkdir data\n",
        "!wget https://raw.githubusercontent.com/timothelaborie/text_classification_scripts/main/data/finance_sentiment_multiclass.csv -P data/"
      ],
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kaggle": {
      "accelerator": "gpu",
      "dataSources": [
        {
          "datasetId": 5081962,
          "sourceId": 8512897,
          "sourceType": "datasetVersion"
        }
      ],
      "dockerImageVersionId": 30733,
      "isGpuEnabled": true,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook"
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "6c29cff5c3b44b47a3c62e532168b0a6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_cc2dfedf5a0b4bf4b58d2e999112d476",
              "IPY_MODEL_6fc2e950e7204b6fa1f25f9d3b2d0a92",
              "IPY_MODEL_28bda91e30d944d39e57f1b9b1bd5b52"
            ],
            "layout": "IPY_MODEL_ab83c0c74bc24e3e96a49a602430a862"
          }
        },
        "cc2dfedf5a0b4bf4b58d2e999112d476": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7e563ea835964ef7b3876f5deab1e725",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_f6ff88e103754871b310ba3f3a433449",
            "value": "Loading‚Äácheckpoint‚Äáshards:‚Äá100%"
          }
        },
        "6fc2e950e7204b6fa1f25f9d3b2d0a92": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0ca2418d9885484ca6dbf3c90ff18183",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_0d142c8937f64fee97e0cad57e3c0be4",
            "value": 2
          }
        },
        "28bda91e30d944d39e57f1b9b1bd5b52": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_49772c6f304544ad86ad9f03c955c213",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_2d86f18e5c3841df95489b5de7c7411c",
            "value": "‚Äá2/2‚Äá[00:44&lt;00:00,‚Äá21.47s/it]"
          }
        },
        "ab83c0c74bc24e3e96a49a602430a862": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7e563ea835964ef7b3876f5deab1e725": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f6ff88e103754871b310ba3f3a433449": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "0ca2418d9885484ca6dbf3c90ff18183": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0d142c8937f64fee97e0cad57e3c0be4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "49772c6f304544ad86ad9f03c955c213": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2d86f18e5c3841df95489b5de7c7411c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "5c68dd49b2354d2da961d3d5d4a864a3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_f1849fc96a074fb392d11e8559fcd063",
              "IPY_MODEL_fd86ec86375a403c87a617fa9f56d685",
              "IPY_MODEL_6b4d67b80e314e26b9420cd6b83bdd8b"
            ],
            "layout": "IPY_MODEL_382c8cdb24554f4db11b9da6eb057ab1"
          }
        },
        "f1849fc96a074fb392d11e8559fcd063": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_52c69c3af7504294bee8d85ecc0a963a",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_fcce99d90452441a957846199e4e2d96",
            "value": "Unsloth:‚ÄáTokenizing‚Äá[&quot;text&quot;]‚Äá(num_proc=2):‚Äá100%"
          }
        },
        "fd86ec86375a403c87a617fa9f56d685": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2d51a6b7560e45d4b31a33f422446efa",
            "max": 3893,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_106ded37e9db4c19aba77b41de3eae02",
            "value": 3893
          }
        },
        "6b4d67b80e314e26b9420cd6b83bdd8b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ccca9d950c8240c583dad8e9005f79c0",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_cc38ef1594d9422189d15d20bcf44229",
            "value": "‚Äá3893/3893‚Äá[00:04&lt;00:00,‚Äá1531.38‚Äáexamples/s]"
          }
        },
        "382c8cdb24554f4db11b9da6eb057ab1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "52c69c3af7504294bee8d85ecc0a963a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fcce99d90452441a957846199e4e2d96": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "2d51a6b7560e45d4b31a33f422446efa": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "106ded37e9db4c19aba77b41de3eae02": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "ccca9d950c8240c583dad8e9005f79c0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cc38ef1594d9422189d15d20bcf44229": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}