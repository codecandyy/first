{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yzrOIcNbnVgY"
      },
      "source": [
        "To run this, press \"*Runtime*\" and press \"*Run all*\" on a **free** Tesla T4 Google Colab instance!\n",
        "<div class=\"align-center\">\n",
        "<a href=\"https://unsloth.ai/\"><img src=\"https://github.com/unslothai/unsloth/raw/main/images/unsloth%20new%20logo.png\" width=\"115\"></a>\n",
        "<a href=\"https://discord.gg/unsloth\"><img src=\"https://github.com/unslothai/unsloth/raw/main/images/Discord button.png\" width=\"145\"></a>\n",
        "<a href=\"https://docs.unsloth.ai/\"><img src=\"https://github.com/unslothai/unsloth/blob/main/images/documentation%20green%20button.png?raw=true\" width=\"125\"></a></a> Join Discord if you need help + ‚≠ê <i>Star us on <a href=\"https://github.com/unslothai/unsloth\">Github</a> </i> ‚≠ê\n",
        "</div>\n",
        "\n",
        "To install Unsloth on your own computer, follow the installation instructions on our Github page [here](https://docs.unsloth.ai/get-started/installing-+-updating).\n",
        "\n",
        "You will learn how to do [data prep](#Data), how to [train](#Train), how to [run the model](#Inference), & [how to save it](#Save)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZppM2UflnVgb"
      },
      "source": [
        "### News"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a5mojalInVgc"
      },
      "source": [
        "\n",
        "Introducing Unsloth [Standby for RL](https://docs.unsloth.ai/basics/memory-efficient-rl): GRPO is now faster, uses 30% less memory with 2x longer context.\n",
        "\n",
        "Gpt-oss fine-tuning now supports 8√ó longer context with 0 accuracy loss. [Read more](https://docs.unsloth.ai/basics/long-context-gpt-oss-training)\n",
        "\n",
        "Unsloth now supports Text-to-Speech (TTS) models. Read our [guide here](https://docs.unsloth.ai/basics/text-to-speech-tts-fine-tuning).\n",
        "\n",
        "Visit our docs for all our [model uploads](https://docs.unsloth.ai/get-started/all-our-models) and [notebooks](https://docs.unsloth.ai/get-started/unsloth-notebooks).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C3wk7M5nnVgc"
      },
      "source": [
        "### Installation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "dqkFWxkVnVgc"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "# We're installing the latest Torch, Triton, OpenAI's Triton kernels, Transformers and Unsloth!\n",
        "!pip install --upgrade -qqq uv\n",
        "try: import numpy; get_numpy = f\"numpy=={numpy.__version__}\"\n",
        "except: get_numpy = \"numpy\"\n",
        "!uv pip install -qqq \\\n",
        "    \"torch>=2.8.0\" \"triton>=3.4.0\" {get_numpy} torchvision bitsandbytes \"transformers>=4.55.3\" \\\n",
        "    \"unsloth_zoo[base] @ git+https://github.com/unslothai/unsloth-zoo\" \\\n",
        "    \"unsloth[base] @ git+https://github.com/unslothai/unsloth\" \\\n",
        "    git+https://github.com/triton-lang/triton.git@05b2c186c1b6c9a08375389d5efe9cb4c401c075#subdirectory=python/triton_kernels\n",
        "!uv pip install transformers==4.55.4"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NJq3z_gYnVgd"
      },
      "source": [
        "### Unsloth"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r2v_X2fA0Df5"
      },
      "source": [
        "We're about to demonstrate the power of the new OpenAI GPT-OSS 20B model through a finetuning example. To use our `MXFP4` inference example, use this [notebook](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/GPT_OSS_MXFP4_(20B)-Inference.ipynb) instead."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 156,
          "referenced_widgets": [
            "48c638fa39ca44fd84211d43b9459253",
            "8ddecc6d296341e587d7f023f72a92f5",
            "8240de58bf524e5ba9f477dbc54e194d",
            "e7ed27712e434c2c9a8bb5b6deb81197",
            "59aac16c96aa44ef8ba9581ab0599588",
            "de3f7bbb0d5948e68b23e9f3bff67f6d",
            "bd16dfb1caa14a2d84b29db3d29f98db",
            "42dd456a36d74553a2cc32cc2417b222",
            "091052322dd946e9bf01a46e1b12aaad",
            "3b5d0bcb42d844b0b5217bc470ee787e",
            "8377929907454679a4d7afbdbe99bc94"
          ]
        },
        "id": "QmUBVEnvCDJv",
        "outputId": "adde302d-b4d1-4db9-ef98-cb70af566d08"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==((====))==  Unsloth 2025.9.1: Fast Qwen2 patching. Transformers: 4.55.4.\n",
            "   \\\\   /|    Tesla T4. Num GPUs = 1. Max memory: 14.741 GB. Platform: Linux.\n",
            "O^O/ \\_/ \\    Torch: 2.8.0+cu126. CUDA: 7.5. CUDA Toolkit: 12.6. Triton: 3.4.0\n",
            "\\        /    Bfloat16 = FALSE. FA [Xformers = None. FA2 = False]\n",
            " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
            "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/5.55G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "48c638fa39ca44fd84211d43b9459253"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "# ‚úÖ 7B 4bit (GPU-only) ‚Äî T4ÏóêÏÑú Í∞ÄÏû• ÏïàÏ†ï\n",
        "import os, torch\n",
        "from unsloth import FastLanguageModel\n",
        "\n",
        "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True,max_split_size_mb:128\"\n",
        "\n",
        "model_name = \"unsloth/Qwen2.5-7B-Instruct-bnb-4bit\"  # ÎòêÎäî \"unsloth/Meta-Llama-3.1-8B-Instruct-bnb-4bit\"\n",
        "max_seq_length = 1024  # Ï≤òÏùåÏóî 1024Î°ú ÏãúÏûë (Î©îÎ™®Î¶¨ Ïó¨Ïú† ÏÉùÍ∏∞Î©¥ Ïò¨Î¶¨ÏÑ∏Ïöî)\n",
        "dtype = None\n",
        "\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name      = model_name,\n",
        "    dtype           = dtype,\n",
        "    max_seq_length  = max_seq_length,\n",
        "    load_in_4bit    = True,\n",
        "    full_finetuning = False,\n",
        "    device_map      = {\"\": 0},   # üîí 4bitÎäî Ïò§ÌîÑÎ°úÎî© Î∂àÍ∞Ä ‚Üí GPUÎßå ÏÇ¨Ïö©\n",
        ")\n",
        "\n",
        "print(\"‚úÖ Loaded:\", model_name, \"| device:\", getattr(model, \"device\", \"cuda:0\"))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rVqtZVxxnVgf"
      },
      "source": [
        "We now add LoRA adapters for parameter efficient finetuning - this allows us to only efficiently train 1% of all parameters."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f_LK81NRnVgg"
      },
      "outputs": [],
      "source": [
        "model = FastLanguageModel.get_peft_model(\n",
        "    model,\n",
        "    r = 8, # Choose any number > 0 ! Suggested 8, 16, 32, 64, 128\n",
        "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
        "                      \"gate_proj\", \"up_proj\", \"down_proj\",],\n",
        "    lora_alpha = 16,\n",
        "    lora_dropout = 0, # Supports any, but = 0 is optimized\n",
        "    bias = \"none\",    # Supports any, but = \"none\" is optimized\n",
        "    # [NEW] \"unsloth\" uses 30% less VRAM, fits 2x larger batch sizes!\n",
        "    use_gradient_checkpointing = \"unsloth\", # True or \"unsloth\" for very long context\n",
        "    random_state = 3407,\n",
        "    use_rslora = False,  # We support rank stabilized LoRA\n",
        "    loftq_config = None, # And LoftQ\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# === Collect 10 wrong answers from GSM8K (test split)\n",
        "#     ‚Äî robust to format/token-length; only truly-wrong items are kept ===\n",
        "import re, json, random, torch\n",
        "from fractions import Fraction\n",
        "from decimal import Decimal, InvalidOperation\n",
        "\n",
        "# =========================\n",
        "# PRE-FLIGHT (Î∞òÎìúÏãú Ìè¨Ìï®)\n",
        "# =========================\n",
        "# 1) Î™®Îç∏/ÌÜ†ÌÅ¨ÎÇòÏù¥Ï†Ä Ï°¥Ïû¨ ÌôïÏù∏\n",
        "if \"model\" not in globals() or \"tokenizer\" not in globals() or model is None:\n",
        "    raise RuntimeError(\"Î®ºÏ†Ä Î™®Îç∏ Î°úÎî© ÏÖÄÏùÑ Ïã§ÌñâÌï¥ Ï£ºÏÑ∏Ïöî. (model, tokenizer ÌïÑÏöî)\")\n",
        "\n",
        "# 2) model.device Î≥¥Ï†ï (HF Ìè¥Î∞± ÎåÄÎπÑ)\n",
        "if not hasattr(model, \"device\"):\n",
        "    try:\n",
        "        model.device = next(model.parameters()).device\n",
        "    except Exception:\n",
        "        pass\n",
        "\n",
        "# 3) chat template Ìè¥Î∞± (HF ÌÜ†ÌÅ¨ÎÇòÏù¥Ï†ÄÏùº Îïå ÎåÄÎπÑ)\n",
        "if not hasattr(tokenizer, \"apply_chat_template\") or tokenizer.apply_chat_template is None:\n",
        "    def _simple_chat_template(messages, add_generation_prompt=True,\n",
        "                              return_tensors=\"pt\", return_dict=True, **kw):\n",
        "        # Mistral/Qwen instruct Ïä§ÌÉÄÏùºÏùò ÏïÑÏ£º Îã®ÏàúÌïú ÌÖúÌîåÎ¶ø\n",
        "        sys = \"\"\n",
        "        text = \"\"\n",
        "        msgs = list(messages)\n",
        "        if msgs and msgs[0].get(\"role\") == \"system\":\n",
        "            sys = (msgs[0].get(\"content\") or \"\").strip()\n",
        "            msgs = msgs[1:]\n",
        "\n",
        "        for m in msgs:\n",
        "            role = m.get(\"role\")\n",
        "            content = (m.get(\"content\") or \"\").strip()\n",
        "            if role == \"user\":\n",
        "                if sys:\n",
        "                    content = f\"<<SYS>>\\n{sys}\\n<</SYS>>\\n\\n{content}\"\n",
        "                    sys = \"\"\n",
        "                text += f\"[INST] {content} [/INST]\"\n",
        "            elif role == \"assistant\":\n",
        "                text += content\n",
        "\n",
        "        enc = tokenizer(text, return_tensors=return_tensors)\n",
        "        if return_dict:\n",
        "            return enc\n",
        "        return enc.input_ids\n",
        "    tokenizer.apply_chat_template = _simple_chat_template\n",
        "\n",
        "# 4) Unsloth BF16 autocastÎ°ú Ïù∏Ìïú dtype Ï∂©Îèå Î∞©ÏßÄÏö© ÏïàÏ†Ñ generate\n",
        "def _safe_generate_call(model, **kwargs):\n",
        "    \"\"\"\n",
        "    UnslothÍ∞Ä ÍµêÏ≤¥Ìïú fast_generateÎ•º Ïö∞ÌöåÌïòÏó¨,\n",
        "    ÏõêÎûò HF generate(_old_generate)Î•º BF16 autocast OFFÎ°ú Ìò∏Ï∂ú.\n",
        "    \"\"\"\n",
        "    gen_fn = getattr(model, \"_old_generate\", None)\n",
        "    if not callable(gen_fn):\n",
        "        gen_fn = model.generate\n",
        "    # eos ÏïàÏ†Ñ Ï£ºÏûÖ\n",
        "    eos_id = getattr(tokenizer, \"eos_token_id\", None)\n",
        "    if eos_id is not None and \"eos_token_id\" not in kwargs:\n",
        "        kwargs[\"eos_token_id\"] = eos_id\n",
        "    try:\n",
        "        # Updated autocast call\n",
        "        with torch.amp.autocast(device_type=\"cuda\", enabled=False):\n",
        "            return gen_fn(**kwargs)\n",
        "    except Exception:\n",
        "        return gen_fn(**kwargs)\n",
        "\n",
        "# =========================\n",
        "# Ïú†Ìã∏/ÌååÏÑú (Ìè¨Îß∑/Í∏∏Ïù¥ ÏïàÏ†Ñ)\n",
        "# =========================\n",
        "def _clean(s): return (s or \"\").strip()\n",
        "\n",
        "BOXED_RE = re.compile(r\"\\\\boxed\\{([^}]*)\\}\")\n",
        "HASH_RE  = re.compile(r\"####\\s*([^\\n]+)\")\n",
        "FINAL_RE = re.compile(r\"(?:final answer|answer is|ans(?:wer)?\\:?)\\s*[:\\-]?\\s*([^\\n]+)\", re.I)\n",
        "NUMBER_RE = re.compile(r\"-?\\d+/\\d+|-?\\d+(?:\\.\\d+)?\")  # Î∂ÑÏàò ÎòêÎäî ÏÜåÏàò/Ï†ïÏàò\n",
        "\n",
        "def normalize(s: str) -> str:\n",
        "    s = _clean(str(s))\n",
        "    m = BOXED_RE.search(s)\n",
        "    if m: s = m.group(1)\n",
        "    s = re.sub(r\"[,\\s]+\",\"\",s)\n",
        "    s = re.sub(r\"[.:;]+$\",\"\",s)\n",
        "    return s.lower()\n",
        "\n",
        "def extract_final(text: str) -> str:\n",
        "    t = _clean(text)\n",
        "    for rx in (HASH_RE, BOXED_RE, FINAL_RE):\n",
        "        m = rx.search(t)\n",
        "        if m: return _clean(m.group(1))\n",
        "    nums = NUMBER_RE.findall(t)\n",
        "    if nums: return _clean(nums[-1])  # Î≥∏Î¨∏ ÎÇ¥ ÎßàÏßÄÎßâ ÏàòÎ•º Î∞±ÏóÖ Ï†ïÎãµÏúºÎ°ú\n",
        "    lines = [l for l in t.splitlines() if _clean(l)]\n",
        "    return _clean(lines[-1]) if lines else t\n",
        "\n",
        "def candidates_from_text(text: str):\n",
        "    return [normalize(x) for x in NUMBER_RE.findall(text or \"\")]\n",
        "\n",
        "def as_fraction(x: str):\n",
        "    x = _clean(x)\n",
        "    try:\n",
        "        if \"/\" in x: return Fraction(x)\n",
        "        return Fraction(Decimal(x))\n",
        "    except (InvalidOperation, ZeroDivisionError, ValueError):\n",
        "        return None\n",
        "\n",
        "def numerically_equal(a: str, b: str) -> bool:\n",
        "    fa, fb = as_fraction(a), as_fraction(b)\n",
        "    return (fa is not None and fb is not None and fa == fb)\n",
        "\n",
        "def is_correct(pred_text: str, gold_text: str) -> bool:\n",
        "    pred_final = extract_final(pred_text)\n",
        "    gold = normalize(gold_text)\n",
        "    pred_norm = normalize(pred_final)\n",
        "    if pred_norm and gold and (pred_norm == gold or numerically_equal(pred_norm, gold)):\n",
        "        return True\n",
        "    cand = candidates_from_text(pred_text)\n",
        "    if gold in cand:\n",
        "        return True\n",
        "    for c in cand:\n",
        "        if numerically_equal(c, gold):\n",
        "            return True\n",
        "    return False\n",
        "\n",
        "def truncate_after_hashline(text: str) -> str:\n",
        "    t = text or \"\"\n",
        "    m = HASH_RE.search(t)\n",
        "    if not m: return t\n",
        "    end = t.find(\"\\n\", m.start())\n",
        "    return t if end == -1 else t[:end]\n",
        "\n",
        "# =========================\n",
        "# ÏÉùÏÑ± ÎûòÌçº (ÏµúÏ¢Ö Ï§Ñ Í∞ïÏ†ú: \"#### <Ïà´Ïûê>\")\n",
        "# =========================\n",
        "def ask_only_new_tokens(question, effort=\"medium\", max_new_tokens=256):\n",
        "    sys_prompt = (\n",
        "        \"reasoning language: English\\n\"\n",
        "        \"You are a helpful math assistant.\\n\"\n",
        "        \"Show concise reasoning, THEN end with exactly ONE final line:\\n\"\n",
        "        \"#### <final numeric answer>\\n\"\n",
        "        \"No words, units, or extra lines after that final line.\"\n",
        "    )\n",
        "    msgs = [\n",
        "        {\"role\":\"system\",\"content\":sys_prompt},\n",
        "        {\"role\":\"user\",\"content\": question},\n",
        "    ]\n",
        "    inputs = tokenizer.apply_chat_template(\n",
        "        msgs,\n",
        "        add_generation_prompt=True,\n",
        "        return_tensors=\"pt\",\n",
        "        return_dict=True,\n",
        "        reasoning_effort=effort,\n",
        "    ).to(model.device)\n",
        "\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        outputs = _safe_generate_call(\n",
        "            model,\n",
        "            **inputs,\n",
        "            max_new_tokens=max_new_tokens,\n",
        "            do_sample=False,\n",
        "        )\n",
        "\n",
        "    gen_ids = outputs[:, inputs[\"input_ids\"].shape[1]:]\n",
        "    pred_text = tokenizer.batch_decode(gen_ids, skip_special_tokens=True)[0]\n",
        "    pred_text = truncate_after_hashline(pred_text)\n",
        "\n",
        "    # Î∞±ÏóÖ: '####'Í∞Ä ÏóÜÏúºÎ©¥ Ï¥àÍ∞ÑÎã® Î™®ÎìúÎ°ú Ïû¨ÏãúÎèÑ\n",
        "    if \"####\" not in pred_text:\n",
        "        strict_msgs = [\n",
        "            {\"role\":\"system\",\"content\":\"Output only the final numeric answer on one line prefixed by '#### '. No other text.\"},\n",
        "            {\"role\":\"user\",\"content\": question},\n",
        "        ]\n",
        "        strict_inputs = tokenizer.apply_chat_template(\n",
        "            strict_msgs,\n",
        "            add_generation_prompt=True,\n",
        "            return_tensors=\"pt\",\n",
        "            return_dict=True,\n",
        "            reasoning_effort=\"low\",\n",
        "        ).to(model.device)\n",
        "        with torch.no_grad():\n",
        "            strict_out = _safe_generate_call(\n",
        "                model,\n",
        "                **strict_inputs,\n",
        "                max_new_tokens=64,\n",
        "                do_sample=False,\n",
        "            )\n",
        "        gen_ids2 = strict_out[:, strict_inputs[\"input_ids\"].shape[1]:]\n",
        "        pred_text2 = tokenizer.batch_decode(gen_ids2, skip_special_tokens=True)[0]\n",
        "        pred_text = truncate_after_hashline(pred_text2) or pred_text\n",
        "\n",
        "    return pred_text\n",
        "\n",
        "# =========================\n",
        "# Îç∞Ïù¥ÌÑ∞ Î°úÎìú & Ïò§Îãµ 10Í∞ú ÏàòÏßë\n",
        "# =========================\n",
        "try:\n",
        "    from datasets import load_dataset\n",
        "except Exception:\n",
        "    raise RuntimeError(\"`datasets` Í∞Ä ÌïÑÏöîÌï©ÎãàÎã§. ÏÑ§Ïπò ÏÖÄÏùÑ Î®ºÏ†Ä Ïã§ÌñâÌïòÏÑ∏Ïöî.\")\n",
        "\n",
        "ds = load_dataset(\"openai/gsm8k\", \"main\", split=\"test\")\n",
        "\n",
        "random.seed(3407)\n",
        "indices = list(range(len(ds)))\n",
        "random.shuffle(indices)\n",
        "\n",
        "wrong = []\n",
        "for i in indices:\n",
        "    q   = _clean(ds[i][\"question\"])\n",
        "    sol = _clean(ds[i][\"answer\"])\n",
        "    gold = extract_final(sol)\n",
        "\n",
        "    pred_raw = ask_only_new_tokens(q, effort=\"medium\", max_new_tokens=256)\n",
        "\n",
        "    # ‚ú® Ìè¨Îß∑/ÏûòÎ¶ºÍ≥º Î¨¥Í¥ÄÌïòÍ≤å 'ÏàòÌïôÏ†ÅÏúºÎ°ú Ï†ïÎãµ'Ïù¥Î©¥ Ïä§ÌÇµ\n",
        "    if is_correct(pred_raw, gold):\n",
        "        # ÏßÑÌñâÏÉÅÌô© Î≥¥Í≥† Ïã∂ÏúºÎ©¥ ÏïÑÎûò Ï£ºÏÑù Ìï¥Ï†ú\n",
        "        # print(f\"[GSM8K] idx={i} correct ‚Äî skipped\")\n",
        "        continue\n",
        "\n",
        "    pred_final = extract_final(pred_raw)\n",
        "    wrong.append({\n",
        "        \"dataset\": \"GSM8K\",\n",
        "        \"idx\": i,\n",
        "        \"question\": q,\n",
        "        \"solution\": sol,        # SFT ÌÉÄÍ≤üÏúºÎ°ú Ïì∞Í∏∞ Ï¢ãÏùå(Ìï¥ÏÑ§ Ï†ÑÏ≤¥)\n",
        "        \"gold\": gold,\n",
        "        \"pred_before\": pred_final,\n",
        "        \"raw_before\": pred_raw,\n",
        "        \"gen\": {\"effort\":\"medium\",\"max_new_tokens\":256}\n",
        "    })\n",
        "    print(f\"[GSM8K] collected wrong #{len(wrong)} (idx={i})\")\n",
        "\n",
        "    if len(wrong) >= 10:\n",
        "        break\n",
        "\n",
        "# Ï†ÄÏû•\n",
        "out_path = \"wrong_gsm8k_10.jsonl\"\n",
        "with open(out_path, \"w\", encoding=\"utf-8\") as f:\n",
        "    for r in wrong:\n",
        "        f.write(json.dumps(r, ensure_ascii=False) + \"\\n\")\n",
        "\n",
        "print(f\"‚úÖ Saved {len(wrong)} items to {out_path}\")"
      ],
      "metadata": {
        "id": "88MigLQ6LjMN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4-sFShVvnVgg"
      },
      "source": [
        "### Reasoning Effort\n",
        "The `gpt-oss` models from OpenAI include a feature that allows users to adjust the model's \"reasoning effort.\" This gives you control over the trade-off between the model's performance and its response speed (latency) which by the amount of token the model will use to think.\n",
        "\n",
        "----\n",
        "\n",
        "The `gpt-oss` models offer three distinct levels of reasoning effort you can choose from:\n",
        "\n",
        "* **Low**: Optimized for tasks that need very fast responses and don't require complex, multi-step reasoning.\n",
        "* **Medium**: A balance between performance and speed.\n",
        "* **High**: Provides the strongest reasoning performance for tasks that require it, though this results in higher latency."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yxCi64FnnVgh"
      },
      "outputs": [],
      "source": [
        "from transformers import TextStreamer\n",
        "\n",
        "messages = [\n",
        "    {\"role\": \"user\", \"content\": \"Solve x^5 + 3x^4 - 10 = 3.\"},\n",
        "]\n",
        "inputs = tokenizer.apply_chat_template(\n",
        "    messages,\n",
        "    add_generation_prompt = True,\n",
        "    return_tensors = \"pt\",\n",
        "    return_dict = True,\n",
        "    reasoning_effort = \"low\", # **NEW!** Set reasoning effort to low, medium or high\n",
        ").to(model.device)\n",
        "\n",
        "_ = model.generate(**inputs, max_new_tokens = 64, streamer = TextStreamer(tokenizer))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IlAzq_RinVgh"
      },
      "source": [
        "Changing the `reasoning_effort` to `medium` will make the model think longer. We have to increase the `max_new_tokens` to occupy the amount of the generated tokens but it will give better and more correct answer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kaPPyXN1nVgh"
      },
      "outputs": [],
      "source": [
        "from transformers import TextStreamer\n",
        "\n",
        "messages = [\n",
        "    {\"role\": \"user\", \"content\": \"Solve x^5 + 3x^4 - 10 = 3.\"},\n",
        "]\n",
        "inputs = tokenizer.apply_chat_template(\n",
        "    messages,\n",
        "    add_generation_prompt = True,\n",
        "    return_tensors = \"pt\",\n",
        "    return_dict = True,\n",
        "    reasoning_effort = \"medium\", # **NEW!** Set reasoning effort to low, medium or high\n",
        ").to(model.device)\n",
        "\n",
        "_ = model.generate(**inputs, max_new_tokens = 64, streamer = TextStreamer(tokenizer))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M0iuyJt7nVgh"
      },
      "source": [
        "Lastly we will test it using `reasoning_effort` to `high`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QrjUXjN8nVgh"
      },
      "outputs": [],
      "source": [
        "from transformers import TextStreamer\n",
        "\n",
        "messages = [\n",
        "    {\"role\": \"user\", \"content\": \"Solve x^5 + 3x^4 - 10 = 3.\"},\n",
        "]\n",
        "inputs = tokenizer.apply_chat_template(\n",
        "    messages,\n",
        "    add_generation_prompt = True,\n",
        "    return_tensors = \"pt\",\n",
        "    return_dict = True,\n",
        "    reasoning_effort = \"high\", # **NEW!** Set reasoning effort to low, medium or high\n",
        ").to(model.device)\n",
        "\n",
        "_ = model.generate(**inputs, max_new_tokens = 64, streamer = TextStreamer(tokenizer))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e6BnnYcbnVgh"
      },
      "source": [
        "<a name=\"Data\"></a>\n",
        "### Data Prep"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "91gfk9L3nVgh"
      },
      "source": [
        "The `HuggingFaceH4/Multilingual-Thinking` dataset will be utilized as our example. This dataset, available on Hugging Face, contains reasoning chain-of-thought examples derived from user questions that have been translated from English into four other languages. It is also the same dataset referenced in OpenAI's [cookbook](https://cookbook.openai.com/articles/gpt-oss/fine-tune-transfomers) for fine-tuning. The purpose of using this dataset is to enable the model to learn and develop reasoning capabilities in these four distinct languages."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "62QfuPXBnVgi"
      },
      "outputs": [],
      "source": [
        "def formatting_prompts_func(examples):\n",
        "    convos = examples[\"messages\"]\n",
        "    texts = [tokenizer.apply_chat_template(convo, tokenize = False, add_generation_prompt = False) for convo in convos]\n",
        "    return { \"text\" : texts, }\n",
        "pass\n",
        "\n",
        "from datasets import load_dataset\n",
        "\n",
        "dataset = load_dataset(\"HuggingFaceH4/Multilingual-Thinking\", split=\"train\")\n",
        "dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WoVaL_i-nVgj"
      },
      "source": [
        "To format our dataset, we will apply our version of the GPT OSS prompt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FW-l11GBnVgj"
      },
      "outputs": [],
      "source": [
        "from unsloth.chat_templates import standardize_sharegpt\n",
        "dataset = standardize_sharegpt(dataset)\n",
        "dataset = dataset.map(formatting_prompts_func, batched = True,)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F1EmcWNinVgj"
      },
      "source": [
        "Let's take a look at the dataset, and check what the 1st example shows"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GNgkE5QxnVgj"
      },
      "outputs": [],
      "source": [
        "print(dataset[0]['text'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tQ3i-AMFnVgj"
      },
      "source": [
        "What is unique about GPT-OSS is that it uses OpenAI [Harmony](https://github.com/openai/harmony) format which support conversation structures, reasoning output, and tool calling."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rtdsxyl6nVgk"
      },
      "source": [
        "<a name=\"Train\"></a>\n",
        "### Train the model\n",
        "Now let's train our model. We do 60 steps to speed things up, but you can set `num_train_epochs=1` for a full run, and turn off `max_steps=None`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O-XZLeLYnVgk"
      },
      "outputs": [],
      "source": [
        "from trl import SFTConfig, SFTTrainer\n",
        "trainer = SFTTrainer(\n",
        "    model = model,\n",
        "    tokenizer = tokenizer,\n",
        "    train_dataset = dataset,\n",
        "    args = SFTConfig(\n",
        "        per_device_train_batch_size = 1,\n",
        "        gradient_accumulation_steps = 4,\n",
        "        warmup_steps = 5,\n",
        "        # num_train_epochs = 1, # Set this for 1 full training run.\n",
        "        max_steps = 30,\n",
        "        learning_rate = 2e-4,\n",
        "        logging_steps = 1,\n",
        "        optim = \"adamw_8bit\",\n",
        "        weight_decay = 0.01,\n",
        "        lr_scheduler_type = \"linear\",\n",
        "        seed = 3407,\n",
        "        output_dir = \"outputs\",\n",
        "        report_to = \"none\", # Use this for WandB etc\n",
        "    ),\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "BF6so1JTnVgk"
      },
      "outputs": [],
      "source": [
        "# @title Show current memory stats\n",
        "gpu_stats = torch.cuda.get_device_properties(0)\n",
        "start_gpu_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
        "max_memory = round(gpu_stats.total_memory / 1024 / 1024 / 1024, 3)\n",
        "print(f\"GPU = {gpu_stats.name}. Max memory = {max_memory} GB.\")\n",
        "print(f\"{start_gpu_memory} GB of memory reserved.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aFaejiSonVgk"
      },
      "outputs": [],
      "source": [
        "trainer_stats = trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "_G3eBV3EnVgk"
      },
      "outputs": [],
      "source": [
        "# @title Show final memory and time stats\n",
        "used_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
        "used_memory_for_lora = round(used_memory - start_gpu_memory, 3)\n",
        "used_percentage = round(used_memory / max_memory * 100, 3)\n",
        "lora_percentage = round(used_memory_for_lora / max_memory * 100, 3)\n",
        "print(f\"{trainer_stats.metrics['train_runtime']} seconds used for training.\")\n",
        "print(\n",
        "    f\"{round(trainer_stats.metrics['train_runtime']/60, 2)} minutes used for training.\"\n",
        ")\n",
        "print(f\"Peak reserved memory = {used_memory} GB.\")\n",
        "print(f\"Peak reserved memory for training = {used_memory_for_lora} GB.\")\n",
        "print(f\"Peak reserved memory % of max memory = {used_percentage} %.\")\n",
        "print(f\"Peak reserved memory for training % of max memory = {lora_percentage} %.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CuK0hVOsnVgk"
      },
      "source": [
        "<a name=\"Inference\"></a>\n",
        "### Inference\n",
        "Let's run the model! You can change the instruction and input - leave the output blank!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RdVCmTuBnVgl"
      },
      "outputs": [],
      "source": [
        "messages = [\n",
        "    {\"role\": \"system\", \"content\": \"reasoning language: French\\n\\nYou are a helpful assistant that can solve mathematical problems.\"},\n",
        "    {\"role\": \"user\", \"content\": \"Solve x^5 + 3x^4 - 10 = 3.\"},\n",
        "]\n",
        "inputs = tokenizer.apply_chat_template(\n",
        "    messages,\n",
        "    add_generation_prompt = True,\n",
        "    return_tensors = \"pt\",\n",
        "    return_dict = True,\n",
        "    reasoning_effort = \"medium\",\n",
        ").to(model.device)\n",
        "from transformers import TextStreamer\n",
        "_ = model.generate(**inputs, max_new_tokens = 64, streamer = TextStreamer(tokenizer))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<a name=\"Save\"></a>\n",
        "### Saving, loading finetuned models\n",
        "To save the final model as LoRA adapters, either use Huggingface's `push_to_hub` for an online save or `save_pretrained` for a local save.\n",
        "\n",
        "**[NOTE]** Currently finetunes can only be loaded via Unsloth in the meantime - we're working on vLLM and GGUF exporting!"
      ],
      "metadata": {
        "id": "5e1j8KRb4AwO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.save_pretrained(\"finetuned_model\")\n",
        "# model.push_to_hub(\"hf_username/finetuned_model\", token = \"hf_...\") # Save to HF"
      ],
      "metadata": {
        "id": "Ds7ByU7e4KF7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "To run the finetuned model, you can do the below after setting `if False` to `if True` in a new instance."
      ],
      "metadata": {
        "id": "ELyXzRpl4hr0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if False:\n",
        "    from unsloth import FastLanguageModel\n",
        "    model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "        model_name = \"finetuned_model\", # YOUR MODEL YOU USED FOR TRAINING\n",
        "        max_seq_length = 1024,\n",
        "        dtype = None,\n",
        "        load_in_4bit = True,\n",
        "    )\n",
        "\n",
        "messages = [\n",
        "    {\"role\": \"system\", \"content\": \"reasoning language: French\\n\\nYou are a helpful assistant that can solve mathematical problems.\"},\n",
        "    {\"role\": \"user\", \"content\": \"Solve x^5 + 3x^4 - 10 = 3.\"},\n",
        "]\n",
        "inputs = tokenizer.apply_chat_template(\n",
        "    messages,\n",
        "    add_generation_prompt = True,\n",
        "    return_tensors = \"pt\",\n",
        "    return_dict = True,\n",
        "    reasoning_effort = \"high\",\n",
        ").to(model.device)\n",
        "from transformers import TextStreamer\n",
        "_ = model.generate(**inputs, max_new_tokens = 64, streamer = TextStreamer(tokenizer))"
      ],
      "metadata": {
        "id": "kCMDSxvD4SKu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qMNviX7XnVgl"
      },
      "source": [
        "And we're done! If you have any questions on Unsloth, we have a [Discord](https://discord.gg/unsloth) channel! If you find any bugs or want to keep updated with the latest LLM stuff, or need help, join projects etc, feel free to join our Discord!\n",
        "\n",
        "Some other links:\n",
        "1. Train your own reasoning model - Llama GRPO notebook [Free Colab](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Llama3.1_(8B)-GRPO.ipynb)\n",
        "2. Saving finetunes to Ollama. [Free notebook](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Llama3_(8B)-Ollama.ipynb)\n",
        "3. Llama 3.2 Vision finetuning - Radiography use case. [Free Colab](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Llama3.2_(11B)-Vision.ipynb)\n",
        "6. See notebooks for DPO, ORPO, Continued pretraining, conversational finetuning and more on our [documentation](https://docs.unsloth.ai/get-started/unsloth-notebooks)!\n",
        "\n",
        "<div class=\"align-center\">\n",
        "  <a href=\"https://unsloth.ai\"><img src=\"https://github.com/unslothai/unsloth/raw/main/images/unsloth%20new%20logo.png\" width=\"115\"></a>\n",
        "  <a href=\"https://discord.gg/unsloth\"><img src=\"https://github.com/unslothai/unsloth/raw/main/images/Discord.png\" width=\"145\"></a>\n",
        "  <a href=\"https://docs.unsloth.ai/\"><img src=\"https://github.com/unslothai/unsloth/blob/main/images/documentation%20green%20button.png?raw=true\" width=\"125\"></a>\n",
        "\n",
        "  Join Discord if you need help + ‚≠êÔ∏è <i>Star us on <a href=\"https://github.com/unslothai/unsloth\">Github</a> </i> ‚≠êÔ∏è\n",
        "</div>\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "48c638fa39ca44fd84211d43b9459253": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_8ddecc6d296341e587d7f023f72a92f5",
              "IPY_MODEL_8240de58bf524e5ba9f477dbc54e194d",
              "IPY_MODEL_e7ed27712e434c2c9a8bb5b6deb81197"
            ],
            "layout": "IPY_MODEL_59aac16c96aa44ef8ba9581ab0599588"
          }
        },
        "8ddecc6d296341e587d7f023f72a92f5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_de3f7bbb0d5948e68b23e9f3bff67f6d",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_bd16dfb1caa14a2d84b29db3d29f98db",
            "value": "model.safetensors:‚Äá‚Äá14%"
          }
        },
        "8240de58bf524e5ba9f477dbc54e194d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_42dd456a36d74553a2cc32cc2417b222",
            "max": 5547254622,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_091052322dd946e9bf01a46e1b12aaad",
            "value": 803048005
          }
        },
        "e7ed27712e434c2c9a8bb5b6deb81197": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3b5d0bcb42d844b0b5217bc470ee787e",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_8377929907454679a4d7afbdbe99bc94",
            "value": "‚Äá803M/5.55G‚Äá[00:15&lt;00:28,‚Äá164MB/s]"
          }
        },
        "59aac16c96aa44ef8ba9581ab0599588": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "de3f7bbb0d5948e68b23e9f3bff67f6d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bd16dfb1caa14a2d84b29db3d29f98db": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "42dd456a36d74553a2cc32cc2417b222": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "091052322dd946e9bf01a46e1b12aaad": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "3b5d0bcb42d844b0b5217bc470ee787e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8377929907454679a4d7afbdbe99bc94": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}